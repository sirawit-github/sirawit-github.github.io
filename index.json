[{"authors":null,"categories":null,"content":"I am currently a Robotics Software Engineer at AI \u0026amp; Robotics Ventures, a start-up from a petroleum company named PTTEP. My work revolves around pushing the autonomous level of the subsea pipeline inspection robot, by solving some of the challenging problems in computer vision, robotics, and deep learning domain.\nI graduated from Korea Advanced Institute of Science and Technology (KAIST) with an Electrical Engineering major and a Computer Science minor, where I had several interests from computer architecture, CUDA programming, computer vision, to robotics (in chronological order). I am thankful for Vertically Integrated Architecture Research Group for allowing me to participate in the research during my early year, which helped me discovering myself that I was not a good computer architect, but rather that I was interested in writing efficient C/C++ code for hardware like GPUs. I was indebted to Urban Robotics Reserach Group for supporting me throughout the participation in the final year, where I worked on LiDAR-based 3D object detection for a self-driving car competition in South Korea.\nI believe in the future of autonomy and that wakes me up every morning. I am passionate about the digital transformation and about building software infrastructure that will help create the autonomous world.\nOutside of work, I enjoy playing badminton and hiking. I also participate in data science competition occasionally, but I have been super busy since joining the current start-up.\n  Download my resumé.\n","date":1372636800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1372636800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am currently a Robotics Software Engineer at AI \u0026amp; Robotics Ventures, a start-up from a petroleum company named PTTEP. My work revolves around pushing the autonomous level of the subsea pipeline inspection robot, by solving some of the challenging problems in computer vision, robotics, and deep learning domain.","tags":null,"title":"Sirawit Putpat","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://example.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"In this post I will be exploring the TFX and its integration with Kubeflow Pipelines on Google AI Platform.\nThis post is kind of my summarization for my learning purpose.\n1. Dataset 2. Create Clusters             3. Understanding TFX Pipelines In order to understand TFX pipelines, we need to understand some keywords. For full tutorial, refer to TensorFlow\u0026rsquo;s article here.\nArtifact Artifacts are the output of the steps in a TFX pipeline. They can be used by subsequent steps in the pipeline.\nArtifacts must be stongly typed with an artifact type registered in the ML Metadata store. This point is not very clear yet; I need to research and will come back to expand more on this later.\nQuestions\n Where does artifact get stored? What needs to be changed if we run the pipeline on a Cloud?  Parameter Parameters are something that we can set through configuration, instead of hard coding; they are just like the hyperparameters of a ML/DL model.\nComponent Component is an implementation of the task in our pipeline. Components in TFX are composed of\n Component specification: This defines the component\u0026rsquo;s input and output artifacts, and component\u0026rsquo;s parameters. Executor: This implements the real work of a step in the pipeline. Component interface: This packages the component specification and executor for use in a pipeline. (This is not very clear.)  Questions\n Where does the component get run? Do components run in the same environment? Same OS and same dependencies? What if each component requires different dependencies?  Pipeline TensorFlow says that a TFX pipeline is a portable implementation of an ML workflow, as it can be run on different ochestrators, such as: Apache Airflow, Apache Beam, and Kubeflow Pipelines.\nFirst, we build a pipeline, which is of type tfx.orchestration.pipeline.Pipeline:\nfrom tfx.orchestration import pipeline def _create_pipeline() -\u0026gt; pipeline.Pipeline: \u0026quot;\u0026quot;\u0026quot; \u0026quot;\u0026quot;\u0026quot; pass  To select a different ochestration tool, we need to import from tfx.orchestration module.\n# Airflow from tfx.orchestration.airflow.airflow_dag_runner import AirflowDagRunner from tfx.orchestration.airflow.airflow_dag_runner import AirflowPipelineConfig DAG = AirflowDagRunner(AirflowPipelineConfig()).run( _create_pipeline() )  # Kubeflow from tfx.orchestration.kubeflow import kubeflow_dag_runner kubeflow_dag_runner.KubeflowDagRunner().run( create_pipeline() )  4. TFX Custom Components Understanding the custom components will get us far! Refer to TensorFlow\u0026rsquo;s article here.\nTFX components at runtime  When a pipeline runs a TFX component, the component is executed in three phases:\n First, the Driver uses the component specification to retrieve the required artifacts from the metadata store and pass them into the component. Next, the Executor performs the component\u0026rsquo;s work. Then the Publisher uses the component specification and the results from the executor to store the component\u0026rsquo;s outputs in the metadata store.      Types of custom components   Python function-based components\n The specification is completely defined in the Python code. The function\u0026rsquo;s arguments with type annotations describe input artifact, output artifact, and parameters. The function\u0026rsquo;s body defines the component\u0026rsquo;s executor. The component interface is dedined by adding @component decorator.  @component def MyComponent( model: InputArtifact[Model], output: OutputArtifact[Model], threshold: Parameter[int] = 10 ) -\u0026gt; OutputDict(accuracy=float): \u0026quot;\u0026quot;\u0026quot; \u0026quot;\u0026quot;\u0026quot; pass    Container-based components\n This is suitable for building a component with custom runtime environment and dependencies.    Fully custom components\n This is for building a component that is not in the built-in TFX standard components. It lets us build a component by implementing a custom component specification, executor, and component interface classes.    5. Code 6. Pipeline Dashboard                      7. Dataflow                My Thoughts   TFX seems to be built around TensorFlow. Not very sure if it\u0026rsquo;s gonna work with other DL/ML libraries without heavily modifying the TFX components. But if we are in a Google Cloud/TensorFlow ecosystem, stick with it!.\n  Unlike TFX, MLFlow seems to be more general and more open to other DL/ML libraries.\n  References  https://www.tensorflow.org/tfx/tutorials https://www.tensorflow.org/tfx/guide/mlmd  ","date":1633824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633824000,"objectID":"c3e2c33c5d039d2093a44763244ebfac","permalink":"https://example.com/project/07_tfx_google_ai_platform/","publishdate":"2021-10-10T00:00:00Z","relpermalink":"/project/07_tfx_google_ai_platform/","section":"project","summary":"In this post I will be exploring the TFX and its integration with Kubeflow Pipelines on Google AI Platform.\nThis post is kind of my summarization for my learning purpose.","tags":["Machine Learning Engineering"],"title":"Continuous Training with TFX and Kubeflow Pipelines","type":"project"},{"authors":null,"categories":null,"content":"ARV Hackathon 2021 is back with the new and even more challenging problem statements that dare you to find innovative solutions in Cyber Security and Subsea Machine Learning spaces.\nAll tech talents, start-ups, and the next generation innovators are invited to join ARV in creating the innovative technological solutions that will transform the future of Thailand and Southeast Asia.\nStay Tuned!\n","date":1633564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633564800,"objectID":"a0c8df139548cd6aca6c88a6aebc7a0c","permalink":"https://example.com/post/12_arv_hackathon_2021/","publishdate":"2021-10-07T00:00:00Z","relpermalink":"/post/12_arv_hackathon_2021/","section":"post","summary":"ARV Hackathon 2021 is back with the new and even more challenging problem statements that dare you to find innovative solutions in Cyber Security and Subsea Machine Learning spaces.\nAll tech talents, start-ups, and the next generation innovators are invited to join ARV in creating the innovative technological solutions that will transform the future of Thailand and Southeast Asia.","tags":["Data Science","Robotics"],"title":"AI \u0026 Robotics Hackathon 2021","type":"post"},{"authors":null,"categories":null,"content":"Table of Contents  1. 2. 3. 4. 5. 6. Which microservice failed? 7. 8. 9. Notes    -- 1. 2. 3. 4. 5. 6. Which microservice failed? Given the services (nodes in the graph) that alert, the task is to identify which services is the root cause of the failure. A failure is a root cause only when there are no other dependencies which also fail.\ngraph TD; A--\u0026gt;B; A--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;E; E--\u0026gt;D; C--\u0026gt;F;  Input (alerts):\n1 E 3 A C E 2 A D 3 D E F  Example output:\n1 E 1 E 1 D 2 D F  Explanation\nThe input graph (omitted) is the one shown in the diagram above. The input has 4 alerts.\nIn the first case, only service E has an alert. Thus, E is the root cause.\nIn the second alert case, services A, C and E have alerts. Since A depends on C and C depends on E, we don’t consider them the root cause. E has no dependency and has an alert. Thus, E is the root cause.\nIn the third case, services A and D have alerts. Since A depends on D through C, B and E, we still consider that the failure on service D might have caused a failure on service A even if the alerts on services C, B and E did not fire. Thus, D is the root cause.\nIn the fourth case, services D, E and F have alerts. E depends on D, so E is not the root cause. D and F do not depend on any services which failed, so they both are the root causes.\nSolution 1\nThis is probably not an optimal solution, but it came first when I tried. The idea is to construct a reverse graph and find all the descendants. In this case, the time complexity would be $O(A(E + V))$ where $A$ is the number of alerts and $V$ and $E$ is the number of vertices and edges in the graph respectively.\nvoid dfs( const int root, const int start, const vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; adj, // Adjacency list of the reversed graph vector\u0026lt;bool\u0026gt;\u0026amp; visited, unordered_map\u0026lt;int, bool\u0026gt;\u0026amp; root_causes ) { if (root != start) { auto it = root_causes.find(root); if (it != root_causes.end()) { root_causes.erase(it); } } visited[root] = true; for (const auto n : adj[root]) { if (!visited[n]) { dfs(n, start, adj, visited, root_causes); } } } void solve( const vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; adj, // Adjacency list of the reversed graph unordered_map\u0026lt;int, bool\u0026gt; services_with_alert, const vector\u0026lt;string\u0026gt;\u0026amp; service_names ) { unordered_map\u0026lt;int, bool\u0026gt; root_causes = services_with_alert; for (const auto p : services_with_alert) { size_t num_services = service_names.size(); vector\u0026lt;bool\u0026gt; visited(num_services, false); dfs(p.first, p.first, adj, visited, root_causes); } // print out solution cout \u0026lt;\u0026lt; root_causes.size(); for (const auto p : root_causes) { cout \u0026lt;\u0026lt; \u0026quot; \u0026quot; \u0026lt;\u0026lt; service_names[p.first]; } cout \u0026lt;\u0026lt; endl; }  7. 8. 9. Notes  I couldn\u0026rsquo;t seem to find restrictions on sharing or posting questions in the terms (https://codegoda.io/terms-and-conditions/). But if I read it wrong, Dear Agoda, please let me know if it is against the terms.  ","date":1626566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626566400,"objectID":"c32aa7d413f489ace4ab95278c15a04d","permalink":"https://example.com/post/11_codegoda_2021/","publishdate":"2021-07-18T00:00:00Z","relpermalink":"/post/11_codegoda_2021/","section":"post","summary":"Table of Contents  1. 2. 3. 4. 5. 6. Which microservice failed? 7. 8. 9. Notes    -- 1. 2. 3. 4. 5. 6. Which microservice failed? Given the services (nodes in the graph) that alert, the task is to identify which services is the root cause of the failure.","tags":["C++"],"title":"Codegoda 2021","type":"post"},{"authors":null,"categories":null,"content":"","date":1625961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625961600,"objectID":"d748d9a59b40851b5aa8dcb9aa48bc97","permalink":"https://example.com/post/10_fb_ranking/","publishdate":"2021-07-11T00:00:00Z","relpermalink":"/post/10_fb_ranking/","section":"post","summary":"","tags":["Machine Learning","Data Science"],"title":"Facebook’s News Feed ranking algorithm","type":"post"},{"authors":null,"categories":null,"content":"Anchor Boxes            Data Pipeline      References  Mingxing Tan, Ruoming Pang, Quoc V. Le. EfficientDet: Scalable and Efficient Object Detection. CVPR 2020.  ","date":1625875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625875200,"objectID":"4f8a9dee13ed5fd750819cac5bf114ff","permalink":"https://example.com/post/09_automl/","publishdate":"2021-07-10T00:00:00Z","relpermalink":"/post/09_automl/","section":"post","summary":"Anchor Boxes            Data Pipeline      References  Mingxing Tan, Ruoming Pang, Quoc V. Le. EfficientDet: Scalable and Efficient Object Detection.","tags":["Machine Learning","Deep Learning"],"title":"EfficientDet: Towards Scalable Architecture in AutoML","type":"post"},{"authors":null,"categories":null,"content":"Differentiable Architecture Search\n$$ \\begin{aligned} \u0026amp;\u0026amp; \\min_{\\alpha} \u0026amp;\u0026amp;\u0026amp; \\mathcal{L}_{val} (w^{\\ast} (\\alpha), \\alpha) \\\\\n\u0026amp;\u0026amp; \\text{s.t.} \u0026amp;\u0026amp;\u0026amp; w^{\\ast} (\\alpha) = \\text{argmin}_{w} \\space \\mathcal{L}_{train} (w, \\alpha) \\\\\n\\end{aligned} $$\n","date":1624233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624233600,"objectID":"de815e4920f4673a657f71b372a8adef","permalink":"https://example.com/post/08_darts/","publishdate":"2021-06-21T00:00:00Z","relpermalink":"/post/08_darts/","section":"post","summary":"Differentiable Architecture Search\n$$ \\begin{aligned} \u0026amp;\u0026amp; \\min_{\\alpha} \u0026amp;\u0026amp;\u0026amp; \\mathcal{L}_{val} (w^{\\ast} (\\alpha), \\alpha) \\\\\n\u0026amp;\u0026amp; \\text{s.t.} \u0026amp;\u0026amp;\u0026amp; w^{\\ast} (\\alpha) = \\text{argmin}_{w} \\space \\mathcal{L}_{train} (w, \\alpha) \\\\\n\\end{aligned} $$","tags":["Machine Learning","Deep Learning"],"title":"DARTS: Differentiable Architecture Search","type":"post"},{"authors":null,"categories":null,"content":" The most successful ML projects in production (Tesla, iPhone, Amazon drones, Zipline) are where you own the entire stack. They iterate not just ML algorithms but also: 1) how to collect/label data, 2) infrastructure, 3) hardware ML models run on.\n The above is the takeaway, summarized by Chip Huyen, from a CVPR 2021 talk by Andrej Karpathy, the Director of AI at Tesla. Not surprisingly, Tesla, among other big plyers like Waymo and Cruise, is one of companies that stands out from its competition.\nWaymo, another big player, has released its dataset since 2019. The dataset is in TFRecord format, which requires TensorFlow for reading. This makes the tools and data preprocessing pipeline heavily depend on TensorFlow. If we look at KITTI, Lyft, TRI, or Argoverse, all of these release their dataset in a raw, simple format. As someone who used PyTorch more than TensorFlow back in those days, it was a pain inspecting and debugging the tf.data stuff, when eager execution was not available inside tf.data.\nLooking at this a year later, and especially having listened to Andrej Karpathy\u0026rsquo;s CVPR 2021 talk, I start to appreciate and I could probably see why Google\u0026rsquo;s Waymo has released their dataset in TFRecord format. (Ironically though, if I remember correctly, the winner of the Challenge used PyTorch.)\nThe good news is\u0026hellip; we can now use:\n# TensorFlow 2.5 tf.data.experimental.enable_debug_mode()  Enough for the introduction, let\u0026rsquo;s visualize the data.\nVisualizing Camera Data Most of the code is straightforward.\nimport tensorflow as tf import matplotlib.pyplot as plt import matplotlib.patches as patches # Replace FILENAME with tfrecord file dataset = tf.data.TFRecordDataset(FILENAME, compression_type='') for data in dataset: frame = open_dataset.Frame() frame.ParseFromString(bytearray(data.numpy())) plt.figure() # Draw the camera labels. count = 0 for camera_image in frame.images: for camera_labels in frame.camera_labels: # Ignore camera labels that do not correspond to this camera. if camera_labels.name != camera_image.name: continue count += 1 ax = plt.subplot(2, 3, count) # Iterate over the individual labels. for label in camera_labels.labels: # Draw the object bounding box. ax.add_patch(patches.Rectangle( xy=(label.box.center_x - 0.5 * label.box.length, label.box.center_y - 0.5 * label.box.width), width=label.box.length, height=label.box.width, linewidth=1, edgecolor='red', facecolor='none')) # Show the camera image. plt.imshow(tf.image.decode_jpeg(camera_image.image)) plt.title(camera_image.name)) plt.grid(False) plt.axis('off') plt.show() plt.close()  The following shows some of the dataset.\n        Dataset Statistics 3D Labels   (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./lwh_3d_waymo.json\", function(chart) { Plotly.plot('chart-248579361', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./histogram_l.json\", function(chart) { Plotly.plot('chart-283794156', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./histogram_w.json\", function(chart) { Plotly.plot('chart-274598613', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./histogram_h.json\", function(chart) { Plotly.plot('chart-872149536', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  2D Labels   (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./bbox_compare.json\", function(chart) { Plotly.plot('chart-471936582', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  ","date":1624233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624233600,"objectID":"92592a1703d4d9d2315f0f7474cb68c4","permalink":"https://example.com/post/07_explore_waymo_perception/","publishdate":"2021-06-21T00:00:00Z","relpermalink":"/post/07_explore_waymo_perception/","section":"post","summary":"The most successful ML projects in production (Tesla, iPhone, Amazon drones, Zipline) are where you own the entire stack. They iterate not just ML algorithms but also: 1) how to collect/label data, 2) infrastructure, 3) hardware ML models run on.","tags":["Deep Learning","Machine Learning"],"title":"Waymo Open Dataset","type":"post"},{"authors":null,"categories":null,"content":"","date":1615507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615507200,"objectID":"5ac9e3e66ccdfc8e8bf21ab2133da067","permalink":"https://example.com/post/06_cuda_basic/","publishdate":"2021-03-12T00:00:00Z","relpermalink":"/post/06_cuda_basic/","section":"post","summary":"","tags":["Computer Vision","C++"],"title":"Fast Execution with CUDA!","type":"post"},{"authors":null,"categories":null,"content":"Introduction The Oil and Gas industry is one of the most lucrative industries that has a very high operating cost. Cutting costs is therefore a major priority when it comes to this business. In this post, I share some of the mahcine learning (or data science, if you will) applications that I have worked on.\n           (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./pipe.json\", function(chart) { Plotly.plot('chart-593726814', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./series.json\", function(chart) { Plotly.plot('chart-176389452', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();    (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./data.json\", function(chart) { Plotly.plot('chart-385794162', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  ","date":1611532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611532800,"objectID":"ef04e6823313c90266fc3525700bd294","permalink":"https://example.com/post/05_data_sci_pttep_arv/","publishdate":"2021-01-25T00:00:00Z","relpermalink":"/post/05_data_sci_pttep_arv/","section":"post","summary":"Introduction The Oil and Gas industry is one of the most lucrative industries that has a very high operating cost. Cutting costs is therefore a major priority when it comes to this business.","tags":["Machine Learning","Deep Learning","Data Science"],"title":"Data Science \u0026 Machine Learning In Oil And Gas Industry","type":"post"},{"authors":null,"categories":null,"content":"","date":1609718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609718400,"objectID":"11ee20e487b4bc065c651bd0223625c6","permalink":"https://example.com/post/04_sonar_sim/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/post/04_sonar_sim/","section":"post","summary":"","tags":["Computer Vision","CUDA"],"title":"Custom Sonar Simulation in Gazebo","type":"post"},{"authors":null,"categories":null,"content":" \u0026ldquo;We stand on the brink of a technological revolution. Soon, few of us will own our own automobiles and instead will get around in driverless electric vehicles that we summon with the touch of an app. We will be liberated from driving, prevent over 90% of car crashes, provide freedom of mobility to the elderly and disabled, and decrease our dependence on fossil fuels.\u0026rdquo; — The Quest to Build the Driverless Car\n The same applies to oil and gas industry. Although robotic technologies have entered the oil and gas industry for around some time, the quest to build the autonomous underwater vehicle must go on!\nIn this post, I will briefly write about the kind of the things we do at our R\u0026amp;D team. (Please note that I cannot write all the details and so I have skipped some parts.)\nA System What do we need in order to build an autonomous underwater vehicle and its system?\n Hardware Software  Navigation software Perception algorithms Planning algorithms Control algorithms Simulation platform Data analytics software    All the teams (software, electrical, and mechanical) collaboratively design the robot. The physical aspect of the robot is mainly designed by a mechanical engineer team where they need to consider things such as the dynamic model, hydrodynamic model, robot mechanisms, and etc. The electrical engineer team is the ones who design and lay out electrical circuits connecting all components to a system. The software team mostly look at the high level aspect of the robot such as what sensors, what algorithms, how to communicate with the vessel, how many computing units, how to store logging data, and the list goes on.\nHardware      Navigation Software  IMU dead reckoning SLAM Map building  Perception Algorithms Perception algorithms are probably the key to intelligent autonomous vehicles. At ARV, we have developed several algorithms, including but not limited to 2D object detection, 3D object detection, point cloud-related algorithms, etc., in order to tackle the challenges we encountered in subsea robotics.\nWe have also developed several machine learning and deep learning models for automatic pipeline inspection as well. These are used both in online (real-time) and offline (data analytics software).\nPlanning Algorithms  Way point planning Optimal path planning  Control Algorithms Simulation Platform This is probably the testbed of our robotics software development. We use Gazebo as a simulator for realistic simulation, with some custom-implemented sensor plugins that we developed for our own use.\n   Of course, these custom plugins should run very fast for the simulation to run smoothly, so the algorihtms must be hevily optimized. For example, the custom sonar plugin was implemented in CUDA to speed things up.\n   Data Analytics Software The collected data when the robot operates at the seabed must be analyzed in some way. We build our web application where customers can log in to see the analyzed data (as well as raw data) and the generated report.\n  (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./data.json\", function(chart) { Plotly.plot('chart-617389524', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  ","date":1609718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609718400,"objectID":"1af90fdb74f65630ef4413d15d339221","permalink":"https://example.com/post/03_the_quest_to_build_auv/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/post/03_the_quest_to_build_auv/","section":"post","summary":"\u0026ldquo;We stand on the brink of a technological revolution. Soon, few of us will own our own automobiles and instead will get around in driverless electric vehicles that we summon with the touch of an app.","tags":["Robotics","C++"],"title":"The Quest to Build an Autonomous Underwater Vehicle","type":"post"},{"authors":null,"categories":null,"content":"Note: Some details in the dataset have been obfuscated by the provider in order to maintain the data privacy.\nModern recommendation systems are complicated. The Collaborative Filtering, which used to be the state-of-the-art approach during the Netflix Prize competition, is probably not at the forefront of the research or industry anymore. So this is my motivation to explore a more modern approach, following up from the project that I used to work on a very long time ago when I was in the university.\nRecommender systems optimize for different objectives in different contexts. In this dataset, we are interested in predicting the user engagements.\n0. System Architecture 1. Metrics Relative Cross Entropy Area under the Precision Recall Curve 2. Exploratory Data Analysis Type    Account creation time   (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./user_creation_time.json\", function(chart) { Plotly.plot('chart-524879361', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  User Followers    Followings    Language   (function() { let a = setInterval( function() { if ( typeof window.Plotly === 'undefined' ) { return; } clearInterval( a ); Plotly.d3.json(\"./language.json\", function(chart) { Plotly.plot('chart-468725139', chart.data, chart.layout, {responsive: true}); }); }, 500 ); })();  2. Feature Engineering 3. Tree-based Models 4. Deep Learning Models NCF Deep Learning Models References  Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Recsys. 191–198.  ","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"484941a9a30a693d8cdbff307f666294","permalink":"https://example.com/project/06_recsys_dl/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/project/06_recsys_dl/","section":"project","summary":"Note: Some details in the dataset have been obfuscated by the provider in order to maintain the data privacy.\nModern recommendation systems are complicated. The Collaborative Filtering, which used to be the state-of-the-art approach during the Netflix Prize competition, is probably not at the forefront of the research or industry anymore.","tags":["Data Science","Deep Learning"],"title":"Deep Learning based Recommender System","type":"project"},{"authors":null,"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"aad0a311981675e3ea94245daa0492c9","permalink":"https://example.com/project/05_monocular_slam_deep_learning/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/project/05_monocular_slam_deep_learning/","section":"project","summary":"","tags":["Robotics"],"title":"Monocular SLAM with Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"1. Introduction 2. Model Implementation    3. Initial Results 3.1 Single Class Detection Simple Scenes Currently the model is trained on a single class: car. For the following result, green indicates the ground truth labels, and light blue indicates the predicted results.\nIn a simple scene, the model seems to recognize all the car objects:\n      Harder Since the model is trained using only the top view LIDAR data, it is reasonable that the model can miss the cases where thr LIDAR point cloud of the object is sparse:\n            Easy Mistake However, the model still misses some obvious detection, such as in the following scene. Here, the front car in the very middle doesn\u0026rsquo;t get detected.\n      Why Top View? What I think the top view (or bird\u0026rsquo;s eye view) approach can do well is that: It can detect the objects which are occluded in the front camera view. If we look at the following image, the car on the very right of the image is largely occluded:\n      However, viewing the point cloud from the top, these 2 cars are clearly separated in the space, and therefore the model can easily detect the targeted objects:\n   3.2 Multi-Class Detection 4. Final Results 5. My Thoughts 6. References  H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller. Multi-view convolutional neural networks for 3d shaperecognition ICCV, 2015 Bin Yang, Wenjie Luo, and Raquel Urtasun. PIXOR: Real-time 3d object detection from point clouds CVPR, 2018  ","date":1572393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572393600,"objectID":"4c1a88e4f2b712f713fef2ce820d9695","permalink":"https://example.com/post/02_3d_object_detection/","publishdate":"2019-10-30T00:00:00Z","relpermalink":"/post/02_3d_object_detection/","section":"post","summary":"1. Introduction 2. Model Implementation    3. Initial Results 3.1 Single Class Detection Simple Scenes Currently the model is trained on a single class: car. For the following result, green indicates the ground truth labels, and light blue indicates the predicted results.","tags":["Deep Learning"],"title":"Real-time 3D Object Detection from Point Clouds","type":"post"},{"authors":null,"categories":null,"content":"Introduction This is the first-ever post of my blog; so I will give it a try. This post is about things that I went through when I tried to implement a simple monocular visual odometry from scratch. For a programming language, I choose MATLAB because it is easy-to-use and fast for prototyping a project.\nDisclaimer: This is not a state-of-the-art implementation. This simply serves the purpose of learning.\nThe Problem To give a general idea, visual odometry (VO) is an algorithm that aims to recover the path incrementally, by using the visual input from cameras, and hence the name. It can be considered as a sequential structure from motion, as opposed to hierarchical structure from motion. Imagine a robot or an agent, attached with a calibrated camera $C$, moves through an environment and receives the image continuously. The images $I_k, I_{k-1}$ are taken at different time steps $k$ and $k-1$, which corresponds to the camera pose $C_k$ and $C_{k-1}$ respectively. The task of VO is basically to retrieve the transformation matrix $T = \\left[R \\lvert t \\right]$ that relates two camera poses, and concatenate all the transformaitons $T_k$ to get the current camera pose:\n$$ C_{t} = T_{t,t-1}C_{t-1}$$\nGetting Things Up \u0026amp; Running I first have an initialization function vo_initialize.m that takes two image frames, establishing keypoint correspondences between these two frames using KLT feature tracker, estimating relative camera pose, and finally triangulating an initial 3D point cloud landmarks. I admit that these may sound lacking of excitement (as they are something that is well understood in the computer vision community), but they are not easy to implement from scratch in a single sit.\nFeature Detection This is a simple plementation of Harris corner detector. For each pixel $(u,v)$, we calculate a score\n$$R = det(A_{u,v}) - {\\lambda}trace^2(A_{u,v})$$\nwhere\n$$ A_{u,v} = \\begin{bmatrix} \\sum{I^2_{x}} \u0026amp; \\sum{I_{x}I_{y}}\\ \\sum{I_{x}I_{y}} \u0026amp; \\sum{I^2_{y}} \\end{bmatrix} $$\nand $I_x, I_y$ are the image gradients in $x$ and $y$ direction respectively.\nI_x = conv2(img, [-1 0 1; -2 0 2; -1 0 1], 'valid'); I_y = conv2(img, [-1 -2 -1; 0 0 0; 1 2 1], 'valid'); I_xx = double(I_x.^2); I_yy = double(I_y.^2); I_xy = double(I_x.*I_y); I_xx_sum = conv2(I_xx, ones(patch_size), 'valid'); I_yy_sum = conv2(I_yy, ones(patch_size), 'valid'); I_xy_sum = conv2(I_xy, ones(patch_size), 'valid'); pad_size = floor((patch_size+1)/2); scores = (I_xx_sum.*I_yy_sum - I_xy_sum.^2) - lambda*(I_xx_sum + I_yy_sum).^2; scores(scores \u0026lt; 0) = 0; scores = padarray(scores, [pad_size pad_size]);  After calculating the score, we simply select $k$ keypoints with highest scores (with non-maximum suppression).\nscores_pad = padarray(scores, [r r]); score_size = size(scores_pad); keypoints = zeros(2, k); for i = 1:k [~, idx] = max(scores_pad, [], 'all', 'linear'); [row, col] = ind2sub(score_size, idx); keypoints(:, i) = [row; col] - r; scores_pad(row-r:row+r, col-r:col+r) = 0; end  KLT Feature Tracker \u0026hellip;\n Pose Estimation \u0026hellip;\n Triangulation \u0026hellip;\n The result of vo_initialize.m seems reasonable. Good to go!\n   Problems from Previous Implementation \u0026hellip;\nEstimate World Camera Pose \u0026hellip;\n Bundle Adjustment Bundle adjustment is a very cool concept. To put it simply, it is an optimization algorithm used to refine the estimated trajectory.\nIn this implementation, a motion-only bundle adjustment is implemented, which optimizes only the camera orientation $R$ and position $t$. This implies that\n Results Putting it all together, the vo_initialize.m function initializes the VO pipeline, creating initial 3D point landmarks, extracting initial keypoints, and estimating the initial pose of the camera. The vo_process.m sequentially extracting and tracking image features from an image frame, across frames, and simultaneously estimating the pose of the camera. Bundle adjustment is also implemented to refine the estimated pose at each step. Lastly, new 3D points are regularly created as the number of currently tracked keypoints is shrinking over time. The following is the final result.\n  From the video, it is obvious that this is not a state-of-the-art implementation. There are various components that are not implemented. As we can see, the estimated trajectory starts to deviate from the ground truth after some time, due to the scale drift\u0026ndash;a common problem in monocular VO. The estimated trajetory also wiggles slightly, probaly due to the fact that the full bundle adjustment is not implemented. And the most importantly, I did not try to implement a loop closure.\nReflections The task of implementing VO from scratch may sound lacking of excitement. I believe that the conventional pipeline of VO and SLAM is something that is already well-understood in the computer vision community. What I realize is that academic papers usually have missing steps that are left for the readers to figure out. Here, I tried to connect those steps and the result stands as a self-assesment of my understanding.\n","date":1570233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570233600,"objectID":"ba3bda5b31b1496abb0575a5f3c018da","permalink":"https://example.com/post/01_visual_odometry/","publishdate":"2019-10-05T00:00:00Z","relpermalink":"/post/01_visual_odometry/","section":"post","summary":"Introduction This is the first-ever post of my blog; so I will give it a try. This post is about things that I went through when I tried to implement a simple monocular visual odometry from scratch.","tags":["Robotics","Computer Vision"],"title":"Visual Odometry Implementation from Scratch","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://example.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"This is a project where we used reinforcement learning to train the robot to collect balls in a simple environment. The robot is a 4-Mecanum wheeled robot equipped with a camera and a 2D LiDAR sensor. The algorithms were implemented on Intel NUC and Nvidia Jetson TX2 board.\nThis is my attempt at re-writing a write-up because a lot of recruiters ask me how the project was done and I sometimes forget (it was done in 2018).\nOptimal Policy and Optimal Value Functions Bellman optimality equation\nFor optimal value function $v_{*}$:\n$$ \\begin{aligned} v_{*}(s) \u0026amp;= \\max_{a} E \\left[ R_{t+1} + \\gamma v_{*}(S_{t+1}) \\vert S_{t}=s, A_{t}=a \\right] \\\\\n\u0026amp;= \\max_{a} \\sum_{s',r} p(s',r \\vert s,a) \\left[ r + \\gamma v_{*}(s') \\right] \\end{aligned} $$\nFor optimal action-value function $q_{*}$:\n$$ \\begin{aligned} q_{*}(s,a) \u0026amp;= E \\left[ R_{t+1} + \\gamma \\max_{a'} q_{*}(S_{t+1}, a') \\vert S_{t} = s, A_{t} = a \\right] \\\\\n\u0026amp;= \\sum_{s',r} p(s',r \\vert s,a) \\left[ r + \\gamma \\max_{a'} q_{*}(s',a') \\right] \\end{aligned} $$\nMonte Carlo Methods Monte Carlo methods only require a sample of states, actions, and rewards from interaction between the agent and the environment. It is model-free; the probability distributions such as state-transition $p(s' \\vert s,a)$ need not to be known.\nQ-Learning $$ Q(s,a) \\leftarrow (1-\\alpha)Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s',a') \\right] $$\nInitialize Q(s, a) Start with state \u0026quot;s\u0026quot; Loop: Select action \u0026quot;a\u0026quot; with e-greedy Execute action \u0026quot;a\u0026quot;, receive immediate reward \u0026quot;r\u0026quot; and go to state \u0026quot;s'\u0026quot; Q(s, a) = Q(s, a) + alpha*[r + gamma * max{Q(s', a')} - Q(s, a)]  But what if the number of $(s,a)$ pairs are very big? Then it is not practical to keep the table $Q$ for every pair of $(s,a)$. Instead, we could maintain $Q(s,a)$ as a parameterized function. This is where the neural network comes into play.\nQ-Network Training\nWe define the loss as:\n$$ L(\\theta) = \\left( \\left( r + \\gamma \\max_{a'} {Q(s', a' | \\theta }) \\right) - Q(s,a|\\theta) \\right)^2 $$\nOur objective is to find weight $\\theta$ of the network to minimize the loss:\n$$ \\min_{\\theta} L(\\theta) = \\min_{\\theta} \\left[ \\left( r + \\gamma \\max_{a'} {Q(s', a' | \\theta }) - Q(s,a|\\theta) \\right)^2 \\right] $$\nwhere $r$ is the immediate reward that we observe and the term $r + \\gamma \\max_{a'} {Q(s', a' | \\theta })$ is the approximated target.\nConvergence\nHowever, using a neural network to represent the action-value function tends to be unstable due to:\n Correlations between samples Non-stationary targets  How does DeepMind solve this issue?\n Experience replay Separated networks Go deeper (added by myself)     Experience Replay Separated Networks $$ L(\\theta) = \\left( \\left( r + \\gamma \\max_{a'} {Q(s', a' | \\theta^{-} }) \\right) - Q(s,a|\\theta) \\right)^2 $$\nGallery             References  Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015).  ","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"c2b7523c0a20b2b810036418b20b094f","permalink":"https://example.com/project/02_ball_robot/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/project/02_ball_robot/","section":"project","summary":"This is a project where we used reinforcement learning to train the robot to collect balls in a simple environment. The robot is a 4-Mecanum wheeled robot equipped with a camera and a 2D LiDAR sensor.","tags":["Robotics","Reinforcement Learning"],"title":"Autonomous Ball-Collecting Robot using Reinforcement Learning","type":"project"},{"authors":["Sirawit Putpat","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://example.com/publication/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://example.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4336df848c08880f290a753e7b57c7e5","permalink":"https://example.com/project/01_recommendation_system/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/01_recommendation_system/","section":"project","summary":"","tags":["Data Science"],"title":"Recommendation Algorithm using Collaborative Filtering","type":"project"}]