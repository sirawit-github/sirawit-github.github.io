<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Robotics | </title>
    <link>https://example.com/tag/robotics/</link>
      <atom:link href="https://example.com/tag/robotics/index.xml" rel="self" type="application/rss+xml" />
    <description>Robotics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 07 Oct 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Robotics</title>
      <link>https://example.com/tag/robotics/</link>
    </image>
    
    <item>
      <title>AI &amp; Robotics Hackathon 2021</title>
      <link>https://example.com/post/12_arv_hackathon_2021/</link>
      <pubDate>Thu, 07 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/12_arv_hackathon_2021/</guid>
      <description>&lt;p&gt;ARV Hackathon 2021 is back with the new and even more challenging problem statements that dare you to find innovative solutions in Cyber Security and Subsea Machine Learning spaces.&lt;/p&gt;
&lt;p&gt;All tech talents, start-ups, and the next generation innovators are invited to join ARV in creating the innovative technological solutions that will transform the future of Thailand and Southeast Asia.&lt;/p&gt;
&lt;p&gt;Stay Tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Quest to Build an Autonomous Underwater Vehicle</title>
      <link>https://example.com/post/03_the_quest_to_build_auv/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/03_the_quest_to_build_auv/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;We stand on the brink of a technological revolution. Soon, few of us will own our own automobiles and instead will get around in driverless electric vehicles that we summon with the touch of an app. We will be liberated from driving, prevent over 90% of car crashes, provide freedom of mobility to the elderly and disabled, and decrease our dependence on fossil fuels.&amp;rdquo; — The Quest to Build the Driverless Car&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The same applies to oil and gas industry. Although robotic technologies have entered the oil and gas industry for around some time, the quest to build the autonomous underwater vehicle must go on!&lt;/p&gt;
&lt;p&gt;In this post, I will briefly write about the kind of the things we do at our R&amp;amp;D team. (Please note that I cannot write all the details and so I have skipped some parts.)&lt;/p&gt;
&lt;h2 id=&#34;a-system&#34;&gt;A System&lt;/h2&gt;
&lt;p&gt;What do we need in order to build an autonomous underwater vehicle and its system?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hardware&lt;/li&gt;
&lt;li&gt;Software
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#navigation-software&#34;&gt;Navigation software&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#perception-algorithms&#34;&gt;Perception algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;&#34;&gt;Planning algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#control-algorithms&#34;&gt;Control algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simulation-platform&#34;&gt;Simulation platform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-analytics-software&#34;&gt;Data analytics software&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All the teams (software, electrical, and mechanical) collaboratively design the robot. The physical aspect of the robot is mainly designed by a mechanical engineer team where they need to consider things such as the dynamic model, hydrodynamic model, robot mechanisms, and etc. The electrical engineer team is the ones who design and lay out electrical circuits connecting all components to a system. The software team mostly look at the high level aspect of the robot such as what sensors, what algorithms, how to communicate with the vessel, how many computing units, how to store logging data, and the list goes on.&lt;/p&gt;
&lt;h2 id=&#34;hardware&#34;&gt;Hardware&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://example.com/image/post/03/01.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://example.com/image/post/03/02.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;navigation-software&#34;&gt;Navigation Software&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;IMU dead reckoning&lt;/li&gt;
&lt;li&gt;SLAM&lt;/li&gt;
&lt;li&gt;Map building&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;perception-algorithms&#34;&gt;Perception Algorithms&lt;/h2&gt;
&lt;p&gt;Perception algorithms are probably the key to intelligent autonomous vehicles. At ARV, we have developed several algorithms, including but not limited to 2D object detection, 3D object detection, point cloud-related algorithms, etc., in order to tackle the challenges we encountered in subsea robotics.&lt;/p&gt;
&lt;p&gt;We have also developed several machine learning and &lt;a href=&#34;https://sirawit-github.github.io/post/05_data_sci_pttep_arv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deep learning models for automatic pipeline inspection&lt;/a&gt; as well. These are used both in online (real-time) and offline (&lt;a href=&#34;#data-analytics-software&#34;&gt;data analytics software&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;planning-algorithms&#34;&gt;Planning Algorithms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Way point planning&lt;/li&gt;
&lt;li&gt;Optimal path planning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;control-algorithms&#34;&gt;Control Algorithms&lt;/h2&gt;
&lt;h2 id=&#34;simulation-platform&#34;&gt;Simulation Platform&lt;/h2&gt;
&lt;p&gt;This is probably the testbed of our robotics software development. We use &lt;a href=&#34;http://gazebosim.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gazebo&lt;/a&gt; as a simulator for realistic simulation, with some custom-implemented sensor plugins that we developed for our own use.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://example.com/image/post/04/simulation_old.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Of course, these custom plugins should run very fast for the simulation to run smoothly, so the algorihtms must be hevily optimized. For example, the custom sonar plugin was implemented in CUDA to speed things up.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://example.com/image/post/06/cuda.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;data-analytics-software&#34;&gt;Data Analytics Software&lt;/h2&gt;
&lt;p&gt;The collected data when the robot operates at the seabed must be analyzed in some way. We build our web application where customers can log in to see the analyzed data (as well as raw data) and the generated report.&lt;/p&gt;


&lt;div id=&#34;chart-617389524&#34; class=&#34;chart&#34;&gt;&lt;/div&gt;
&lt;script&gt;
  (function() {
    let a = setInterval( function() {
      if ( typeof window.Plotly === &#39;undefined&#39; ) {
        return;
      }
      clearInterval( a );

      Plotly.d3.json(&#34;./data.json&#34;, function(chart) {
        Plotly.plot(&#39;chart-617389524&#39;, chart.data, chart.layout, {responsive: true});
      });
    }, 500 );
  })();
&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Monocular SLAM with Deep Learning</title>
      <link>https://example.com/project/05_monocular_slam_deep_learning/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/05_monocular_slam_deep_learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Visual Odometry Implementation from Scratch</title>
      <link>https://example.com/post/01_visual_odometry/</link>
      <pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/01_visual_odometry/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is the first-ever post of my blog; so I will give it a try. This post is about things that I went through when I tried to implement a simple monocular visual odometry from scratch. For a programming language, I choose MATLAB because it is easy-to-use and fast for prototyping a project.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt;&lt;/em&gt; This is not a state-of-the-art implementation. This simply serves the purpose of learning.&lt;/p&gt;
&lt;h2 id=&#34;the-problem&#34;&gt;The Problem&lt;/h2&gt;
&lt;p&gt;To give a general idea, visual odometry (VO) is an algorithm that aims to recover the path incrementally, by using the visual input from cameras, and hence the name. It can be considered as a sequential structure from motion, as opposed to hierarchical structure from motion. Imagine a robot or an agent, attached with a calibrated camera $C$, moves through an environment and receives the image continuously. The images $I_k, I_{k-1}$ are taken at different time steps $k$ and $k-1$, which corresponds to the camera pose $C_k$ and $C_{k-1}$ respectively. The task of VO is basically to retrieve the transformation matrix $T = \left[R \lvert t \right]$ that relates two camera poses, and concatenate all the transformaitons $T_k$ to get the current camera pose:&lt;/p&gt;
&lt;p&gt;$$ C_{t} = T_{t,t-1}C_{t-1}$$&lt;/p&gt;
&lt;h2 id=&#34;getting-things-up--running&#34;&gt;Getting Things Up &amp;amp; Running&lt;/h2&gt;
&lt;p&gt;I first have an initialization function &lt;code&gt;vo_initialize.m&lt;/code&gt; that takes two image frames, establishing keypoint correspondences between these two frames using KLT feature tracker, estimating relative camera pose, and finally triangulating an initial 3D point cloud landmarks. I admit that these may sound lacking of excitement (as they are something that is well understood in the computer vision community), but they are not easy to implement from scratch in a single sit.&lt;/p&gt;
&lt;h3 id=&#34;feature-detection&#34;&gt;Feature Detection&lt;/h3&gt;
&lt;p&gt;This is a simple plementation of Harris corner detector. For each pixel $(u,v)$, we calculate a score&lt;/p&gt;
&lt;p&gt;$$R = det(A_{u,v}) - {\lambda}trace^2(A_{u,v})$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$ A_{u,v} = \begin{bmatrix} \sum{I^2_{x}} &amp;amp; \sum{I_{x}I_{y}}\ \sum{I_{x}I_{y}} &amp;amp; \sum{I^2_{y}} \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;and $I_x, I_y$ are the image gradients in $x$ and $y$ direction respectively.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;I_x = conv2(img, [-1 0 1; -2 0 2; -1 0 1], &#39;valid&#39;);
I_y = conv2(img, [-1 -2 -1; 0 0 0; 1 2 1], &#39;valid&#39;);
I_xx = double(I_x.^2);
I_yy = double(I_y.^2);
I_xy = double(I_x.*I_y);
I_xx_sum = conv2(I_xx, ones(patch_size), &#39;valid&#39;);
I_yy_sum = conv2(I_yy, ones(patch_size), &#39;valid&#39;);
I_xy_sum = conv2(I_xy, ones(patch_size), &#39;valid&#39;);

pad_size = floor((patch_size+1)/2);
scores = (I_xx_sum.*I_yy_sum - I_xy_sum.^2) - lambda*(I_xx_sum + I_yy_sum).^2;
scores(scores &amp;lt; 0) = 0;
scores = padarray(scores, [pad_size pad_size]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After calculating the score, we simply select $k$ keypoints with highest scores (with non-maximum suppression).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;scores_pad = padarray(scores, [r r]);
score_size = size(scores_pad);
keypoints = zeros(2, k);
for i = 1:k
    [~, idx] = max(scores_pad, [], &#39;all&#39;, &#39;linear&#39;);
    [row, col] = ind2sub(score_size, idx);
    keypoints(:, i) = [row; col] - r;
    scores_pad(row-r:row+r, col-r:col+r) = 0;
end
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;klt-feature-tracker&#34;&gt;KLT Feature Tracker&lt;/h3&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;pose-estimation&#34;&gt;Pose Estimation&lt;/h3&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;triangulation&#34;&gt;Triangulation&lt;/h3&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result of &lt;code&gt;vo_initialize.m&lt;/code&gt; seems reasonable. Good to go!&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://example.com/image/post/01/post.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;problems-from-previous-implementation&#34;&gt;Problems from Previous Implementation&lt;/h2&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;estimate-world-camera-pose&#34;&gt;Estimate World Camera Pose&lt;/h2&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;bundle-adjustment&#34;&gt;Bundle Adjustment&lt;/h2&gt;
&lt;p&gt;Bundle adjustment is a very cool concept. To put it simply, it is an optimization algorithm used to refine the estimated trajectory.&lt;/p&gt;
&lt;p&gt;In this implementation, a &lt;em&gt;motion-only&lt;/em&gt; bundle adjustment is implemented, which optimizes only the camera orientation $R$ and position $t$. This implies that&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-matlab&#34;&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Putting it all together, the &lt;code&gt;vo_initialize.m&lt;/code&gt; function initializes the VO pipeline, creating initial 3D point landmarks, extracting initial keypoints, and estimating the initial pose of the camera. The &lt;code&gt;vo_process.m&lt;/code&gt; sequentially extracting and tracking image features from an image frame, across frames, and simultaneously estimating the pose of the camera. Bundle adjustment is also implemented to refine the estimated pose at each step. Lastly, new 3D points are regularly created as the number of currently tracked keypoints is shrinking over time. The following is the final result.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/A5HnnSiZ_LM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br&gt;
&lt;p&gt;From the video, it is obvious that this is not a state-of-the-art implementation. There are various components that are not implemented. As we can see, the estimated trajectory starts to deviate from the ground truth after some time, due to the scale drift&amp;ndash;a common problem in monocular VO. The estimated trajetory also wiggles slightly, probaly due to the fact that the full bundle adjustment is not implemented. And the most importantly, I did not try to implement a loop closure.&lt;/p&gt;
&lt;h2 id=&#34;reflections&#34;&gt;Reflections&lt;/h2&gt;
&lt;p&gt;The task of implementing VO from scratch may sound lacking of excitement. I believe that the conventional pipeline of VO and SLAM is something that is already well-understood in the computer vision community. What I realize is that academic papers usually have missing steps that are left for the readers to figure out. Here, I tried to connect those steps and the result stands as a self-assesment of my understanding.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Autonomous Ball-Collecting Robot using Reinforcement Learning</title>
      <link>https://example.com/project/02_ball_robot/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/02_ball_robot/</guid>
      <description>&lt;p&gt;This is a project where we used reinforcement learning to train the robot to collect balls in a simple environment. The robot is a 4-Mecanum wheeled robot equipped with a camera and a 2D LiDAR sensor. The algorithms were implemented on &lt;a href=&#34;https://www.intel.com/content/www/us/en/products/details/nuc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intel NUC&lt;/a&gt; and &lt;a href=&#34;https://developer.nvidia.com/embedded/jetson-tx2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nvidia Jetson TX2&lt;/a&gt; board.&lt;/p&gt;
&lt;p&gt;This is my attempt at re-writing a write-up because a lot of recruiters ask me how the project was done and I sometimes forget (it was done in 2018).&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;optimal-policy-and-optimal-value-functions&#34;&gt;Optimal Policy and Optimal Value Functions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Bellman optimality equation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For optimal value function $v_{*}$:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
v_{*}(s) &amp;amp;= \max_{a} E \left[ R_{t+1} + \gamma v_{*}(S_{t+1}) \vert S_{t}=s, A_{t}=a \right] \\&lt;br&gt;
&amp;amp;= \max_{a} \sum_{s&#39;,r} p(s&#39;,r \vert s,a) \left[ r + \gamma  v_{*}(s&#39;) \right]
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;For optimal action-value function $q_{*}$:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
q_{*}(s,a) &amp;amp;= E \left[ R_{t+1} + \gamma \max_{a&#39;} q_{*}(S_{t+1}, a&#39;) \vert S_{t} = s, A_{t} = a \right] \\&lt;br&gt;
&amp;amp;= \sum_{s&#39;,r} p(s&#39;,r \vert s,a) \left[ r + \gamma \max_{a&#39;} q_{*}(s&#39;,a&#39;) \right]
\end{aligned}
$$&lt;/p&gt;
&lt;h2 id=&#34;monte-carlo-methods&#34;&gt;Monte Carlo Methods&lt;/h2&gt;
&lt;p&gt;Monte Carlo methods only require a sample of states, actions, and rewards from interaction between the agent and the environment. It is model-free; the probability distributions such as state-transition $p(s&#39; \vert s,a)$ need not to be known.&lt;/p&gt;
&lt;h2 id=&#34;q-learning&#34;&gt;Q-Learning&lt;/h2&gt;
&lt;p&gt;$$ Q(s,a) \leftarrow (1-\alpha)Q(s,a) + \alpha \left[ r + \gamma \max_{a&#39;} Q(s&#39;,a&#39;) \right] $$&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Initialize Q(s, a)
Start with state &amp;quot;s&amp;quot;
Loop:
    Select action &amp;quot;a&amp;quot; with e-greedy
    Execute action &amp;quot;a&amp;quot;, receive immediate reward &amp;quot;r&amp;quot; and go to state &amp;quot;s&#39;&amp;quot;
    Q(s, a) = Q(s, a) + alpha*[r + gamma * max{Q(s&#39;, a&#39;)} - Q(s, a)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But what if the number of $(s,a)$ pairs are very big? Then it is not practical to keep the table $Q$ for every pair of $(s,a)$. Instead, we could maintain $Q(s,a)$ as a parameterized function. This is where the neural network comes into play.&lt;/p&gt;
&lt;h2 id=&#34;q-network&#34;&gt;Q-Network&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We define the &lt;strong&gt;loss&lt;/strong&gt; as:&lt;/p&gt;
&lt;p&gt;$$
L(\theta) = \left( \left( r + \gamma \max_{a&#39;} {Q(s&#39;, a&#39; | \theta }) \right) - Q(s,a|\theta) \right)^2
$$&lt;/p&gt;
&lt;p&gt;Our objective is to find weight $\theta$ of the network to minimize the loss:&lt;/p&gt;
&lt;p&gt;$$
\min_{\theta} L(\theta) = \min_{\theta} \left[ \left( r + \gamma \max_{a&#39;} {Q(s&#39;, a&#39; | \theta }) - Q(s,a|\theta) \right)^2 \right]
$$&lt;/p&gt;
&lt;p&gt;where $r$ is the immediate reward that we observe and the term $r + \gamma \max_{a&#39;} {Q(s&#39;, a&#39; | \theta })$ is the approximated target.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Convergence&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;However, using a neural network to represent the action-value function tends to be unstable due to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Correlations between samples&lt;/li&gt;
&lt;li&gt;Non-stationary targets&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How does DeepMind solve this issue?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Experience replay&lt;/li&gt;
&lt;li&gt;Separated networks&lt;/li&gt;
&lt;li&gt;Go deeper (added by myself)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://example.com/image/project/02/go_deeper.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;experience-replay&#34;&gt;Experience Replay&lt;/h2&gt;
&lt;h2 id=&#34;separated-networks&#34;&gt;Separated Networks&lt;/h2&gt;
&lt;p&gt;$$
L(\theta) = \left( \left( r + \gamma \max_{a&#39;} {Q(s&#39;, a&#39; | \theta^{-} }) \right) - Q(s,a|\theta) \right)^2
$$&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;gallery&#34;&gt;Gallery&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://example.com/image/project/02/1.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://example.com/image/project/02/2.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://example.com/image/project/02/3.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://example.com/image/project/02/4.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Mnih, V. et al. &lt;a href=&#34;https://deepmind.com/research/publications/2019/human-level-control-through-deep-reinforcement-learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Human-level control through deep reinforcement learning.&lt;/a&gt; Nature 518, 529–533 (2015).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
